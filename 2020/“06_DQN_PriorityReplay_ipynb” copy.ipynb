{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "“06.DQN_PriorityReplay.ipynb”的副本",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjuJhTUC-oL5",
        "colab_type": "text"
      },
      "source": [
        "# Deep Q Network with Prioritized Experience Replay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZtCnCp--oL6",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-emeb7Nq-oL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym, math, glob, random\n",
        "import numpy as np\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from matplotlib import pyplot as plt\n",
        "get_ipython().run_line_magic('matplotlib', 'inline')\n",
        "\n",
        "from utils.wrappers import *\n",
        "from agents.DQN import Model as DQN_Agent\n",
        "from utils.ReplayMemory import ExperienceReplayMemory\n",
        "\n",
        "from utils.hyperparameters import Config\n",
        "from utils.plot import plot_all_data\n",
        "from utils.data_structures import SegmentTree, MinSegmentTree, SumSegmentTree"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SL7O_j2-oL9",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S8P0Orv-oL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = Config()\n",
        "\n",
        "config.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = config.device\n",
        "\n",
        "#epsilon variables\n",
        "config.epsilon_start = 1.0\n",
        "config.epsilon_final = 0.01\n",
        "config.epsilon_decay = 30000\n",
        "config.epsilon_by_frame = lambda frame_idx: config.epsilon_final + (config.epsilon_start - config.epsilon_final) * math.exp(-1. * frame_idx / config.epsilon_decay)\n",
        "\n",
        "#misc agent variables\n",
        "config.GAMMA=0.99\n",
        "config.LR=1e-4\n",
        "\n",
        "#memory\n",
        "config.TARGET_NET_UPDATE_FREQ = 1000\n",
        "config.EXP_REPLAY_SIZE = 100000\n",
        "config.BATCH_SIZE = 32\n",
        "config.PRIORITY_ALPHA=0.6\n",
        "config.PRIORITY_BETA_START=0.4\n",
        "config.PRIORITY_BETA_FRAMES = 100000\n",
        "\n",
        "#Learning control variables\n",
        "config.LEARN_START = 10000\n",
        "config.MAX_FRAMES=1000000\n",
        "config.UPDATE_FREQ = 1\n",
        "\n",
        "#Nstep controls\n",
        "config.N_STEPS=1\n",
        "\n",
        "#data logging parameters\n",
        "config.ACTION_SELECTION_COUNT_FREQUENCY = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcQodb-Q-oL_",
        "colab_type": "text"
      },
      "source": [
        "## Prioritized Replay (Without Rank-Based Priority)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPX7PEFk-oMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrioritizedReplayMemory(object):\n",
        "    def __init__(self, size, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
        "        super(PrioritizedReplayMemory, self).__init__()\n",
        "        self._storage = []\n",
        "        self._maxsize = size\n",
        "        self._next_idx = 0\n",
        "\n",
        "        assert alpha >= 0\n",
        "        self._alpha = alpha\n",
        "\n",
        "        self.beta_start = beta_start\n",
        "        self.beta_frames = beta_frames\n",
        "        self.frame=1\n",
        "\n",
        "        it_capacity = 1\n",
        "        while it_capacity < size:\n",
        "            it_capacity *= 2\n",
        "\n",
        "        self._it_sum = SumSegmentTree(it_capacity)\n",
        "        self._it_min = MinSegmentTree(it_capacity)\n",
        "        self._max_priority = 1.0\n",
        "\n",
        "    def beta_by_frame(self, frame_idx):\n",
        "        return min(1.0, self.beta_start + frame_idx * (1.0 - self.beta_start) / self.beta_frames)\n",
        "\n",
        "    def push(self, data):\n",
        "        idx = self._next_idx\n",
        "\n",
        "        if self._next_idx >= len(self._storage):\n",
        "            self._storage.append(data)\n",
        "        else:\n",
        "            self._storage[self._next_idx] = data\n",
        "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
        "\n",
        "\n",
        "        self._it_sum[idx] = self._max_priority ** self._alpha\n",
        "        self._it_min[idx] = self._max_priority ** self._alpha\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        return [self._storage[i] for i in idxes]\n",
        "\n",
        "    def _sample_proportional(self, batch_size):\n",
        "        res = []\n",
        "        for _ in range(batch_size):\n",
        "            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n",
        "            idx = self._it_sum.find_prefixsum_idx(mass)\n",
        "            res.append(idx)\n",
        "        return res\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        idxes = self._sample_proportional(batch_size)\n",
        "\n",
        "        weights = []\n",
        "\n",
        "        #find smallest sampling prob: p_min = smallest priority^alpha / sum of priorities^alpha\n",
        "        p_min = self._it_min.min() / self._it_sum.sum()\n",
        "\n",
        "        beta = self.beta_by_frame(self.frame)\n",
        "        self.frame+=1\n",
        "        \n",
        "        #max_weight given to smallest prob\n",
        "        max_weight = (p_min * len(self._storage)) ** (-beta)\n",
        "\n",
        "        for idx in idxes:\n",
        "            p_sample = self._it_sum[idx] / self._it_sum.sum()\n",
        "            weight = (p_sample * len(self._storage)) ** (-beta)\n",
        "            weights.append(weight / max_weight)\n",
        "        weights = torch.tensor(weights, device=config.device, dtype=torch.float) \n",
        "        encoded_sample = self._encode_sample(idxes)\n",
        "        return encoded_sample, idxes, weights\n",
        "\n",
        "    def update_priorities(self, idxes, priorities):\n",
        "        assert len(idxes) == len(priorities)\n",
        "        for idx, priority in zip(idxes, priorities):\n",
        "            assert 0 <= idx < len(self._storage)\n",
        "            self._it_sum[idx] = (priority+1e-5) ** self._alpha\n",
        "            self._it_min[idx] = (priority+1e-5) ** self._alpha\n",
        "\n",
        "            self._max_priority = max(self._max_priority, (priority+1e-5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D_Wilmx-oMC",
        "colab_type": "text"
      },
      "source": [
        "## Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYX8bjAP-oMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(DQN_Agent):\n",
        "    def __init__(self, static_policy=False, env=None, config=None, log_dir='/tmp/gym'):\n",
        "        super(Model, self).__init__(static_policy, env, config, log_dir=log_dir)\n",
        "    \n",
        "    def declare_memory(self):\n",
        "        self.memory = PrioritizedReplayMemory(self.experience_replay_size, self.priority_alpha, self.priority_beta_start, self.priority_beta_frames)\n",
        "        \n",
        "    def compute_loss(self, batch_vars): #faster\n",
        "        batch_state, batch_action, batch_reward, non_final_next_states, non_final_mask, empty_next_state_values, indices, weights = batch_vars\n",
        "\n",
        "        #estimate\n",
        "        self.model.sample_noise()\n",
        "        current_q_values = self.model(batch_state).gather(1, batch_action)\n",
        "        \n",
        "        #target\n",
        "        with torch.no_grad():\n",
        "            max_next_q_values = torch.zeros(self.batch_size, device=self.device, dtype=torch.float).unsqueeze(dim=1)\n",
        "            if not empty_next_state_values:\n",
        "                max_next_action = self.get_max_next_state_action(non_final_next_states)\n",
        "                self.target_model.sample_noise()\n",
        "                max_next_q_values[non_final_mask] = self.target_model(non_final_next_states).gather(1, max_next_action)\n",
        "            expected_q_values = batch_reward + ((self.gamma**self.nsteps)*max_next_q_values)\n",
        "\n",
        "        diff = (expected_q_values - current_q_values)\n",
        "        self.memory.update_priorities(indices, diff.detach().squeeze().abs().cpu().numpy().tolist())\n",
        "        loss = self.MSE(diff).squeeze() * weights\n",
        "        loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NMjVttF-1zR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deep_frames = frames\n",
        "plt.figure(figsize=(deep_frames[0].shape[1] / 72.0, deep_frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(deep_frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(deep_frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(deep_frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR5WPtzD-oME",
        "colab_type": "text"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "ojDxR8GV-oME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "559ee0cb-3511-48dc-b696-6aa0b2eedb7c"
      },
      "source": [
        "start=timer()\n",
        "\n",
        "log_dir = \"/tmp/gym/\"\n",
        "try:\n",
        "    os.makedirs(log_dir)\n",
        "except OSError:\n",
        "    files = glob.glob(os.path.join(log_dir, '*.monitor.csv')) \\\n",
        "        + glob.glob(os.path.join(log_dir, '*td.csv')) \\\n",
        "        + glob.glob(os.path.join(log_dir, '*sig_param_mag.csv')) \\\n",
        "        + glob.glob(os.path.join(log_dir, '*action_log.csv'))\n",
        "    for f in files:\n",
        "        os.remove(f)\n",
        "\n",
        "env_id = \"Pendulum-v0\"\n",
        "env    = make_atari(env_id)\n",
        "env    = bench.Monitor(env, os.path.join(log_dir, env_id))\n",
        "env    = wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=True)\n",
        "env    = WrapPyTorch(env)\n",
        "model  = Model(env=env, config=config, log_dir=log_dir)\n",
        "\n",
        "episode_reward = 0\n",
        "\n",
        "observation = env.reset()\n",
        "for frame_idx in range(1, config.MAX_FRAMES + 1):\n",
        "    epsilon = config.epsilon_by_frame(frame_idx)\n",
        "\n",
        "    action = model.get_action(observation, epsilon)\n",
        "    model.save_action(action, frame_idx) #log action selection\n",
        "\n",
        "    prev_observation=observation\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "    observation = None if done else observation\n",
        "\n",
        "    model.update(prev_observation, action, reward, observation, frame_idx)\n",
        "    episode_reward += reward\n",
        "\n",
        "    if done:\n",
        "        model.finish_nstep()\n",
        "        model.reset_hx()\n",
        "        observation = env.reset()\n",
        "        model.save_reward(episode_reward)\n",
        "        episode_reward = 0\n",
        "\n",
        "    if frame_idx % 10000 == 0:\n",
        "        model.save_w()\n",
        "        try:\n",
        "            clear_output(True)\n",
        "            plot_all_data(log_dir, env_id, 'PriorityDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)\n",
        "        except IOError:\n",
        "            pass\n",
        "\n",
        "model.save_w()\n",
        "env.close()\n",
        "plot_all_data(log_dir, env_id, 'PriorityDQN', config.MAX_FRAMES, bin_size=(10, 100, 100, 1), smooth=1, time=timedelta(seconds=int(timer()-start)), ipynb=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9aa176437349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlog_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/tmp/gym/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'timer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "d4PXkxx6-oMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}